# 9장 - 웹 크롤러 설계 
웹 크롤러는 로봇 또는 스파이더라고도 부른다.       
 검색 엔진에서 널리 쓰는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적이다.
## 웹 크롤러의 이용 
1. 검색 엔진 인덱싱
- 가장 보편적인 용례로 웹페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다. (ex.Googlebot)
2. 웹 아카이빙
- 나중에 사용하기위해 자료 보관 (ex.국립도서관에서 웹 아카이빙으로 정보를 모은다.)
3. 웹 마이닝
- 웹의 폭발적 성장세로 웹 마이닝을 통해 유용한 지식을 얻는다.
4. 웹 모니터링
- 크롤러 저작권이나 상표권 침해 사례를 모니터링 할 수 있다.
-----

## 1단계 - 문제 이해 및 설계 범위 확정

웹크롤러의 기본 알고리즘은 아래와 같다.

1. URL 집합이 입력으로 주어지면 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.
2. 다운받은 URL에서 새로운 URL을 추출한다.
3. 추출된 URL들을 다운로드 할 URL목록에 추가하고 1번부터 다시 반복한다.

---
우리는 규모확장성이 있는 웹 크롤러를 만들어야하며 설계범위를 좁히기위해 면접관에게 요구사항을 물어봐야한다.

1. 주된용도
2. 매월 수집할 웹페이지 수량
3.  새로운 웹페이지 말고도 수정된 웹페이지도 고려해야하는지?
4.  수집한 웹페이지를 얼마동안 보관해야하는지?
5.   중복된 콘테츠를 어떻게다룰지
----
## 2단계 - 개략적 설계안 제시 및 동의 구하기

### 1. 시작URL집합
시작 URL은 출발점을 정한다.       
직관적으론 해당하는 도메인 이름이 붙은 모든 페이지를 URL를 시작URL로 사용하는 것이다.      
하지만 창의적으로 고려하여 가능한 많은 링크 을 탐색할 수 있도록 접근하는 것이 더 좋을 수 있는데      
일반적으로는 전체 URL공간을 작은 부분집합으로 나눈다.     
지역적 특색을 고려하는 것인데, 예시로는 나라별로 인기있는 웹 사이트가 다르다는 점을 고려하는 것이다.     
또 다른 방법으로는 [쇼핑, 스포트, 예술]등등 주제별로 다른 시작URL을 정하는 것이다.     
시작URL을 정하는 법은 정답이 없으니 면접관에게 나의 의도를 명확히 전달하도록 하자.
### 2. 미수집 URL 저장소
대부분 다운로드할 URL/ 다운로드 된 URL로 구분된다.
미수집 URL 저장소는 FIFO형태의 큐라고 생각하면 된다.

### 3.HTML 다운로더
미수집 URL 저장소에서 제공된 웹페이지를 다운로드하는 컴포넌트다.

### 4. 도메인 이름 변환기
웹페이지의 도메인명을 IP로 바꾸는 과정에서 필요하다.

### 5. 콘텐츠 파서 
다운로드된 웹페이지 내용을 파싱 / 검증 절차를 진행한다.
이 과정에서 문제를 일으킬 수 있고 저장공간을 낭비시킬 부분은 제외한다.

### 6. 중복컨텐츠 판별
웹에 공개된 연구결과에 따르면 29%정도는 중복된 컨텐츠이다.
따라서 비교대상 문서를 그저 비교하면 너무 오버헤드가 크므로 보통 해시값 비교로 중복을 판별한다.
<img width="863" height="122" alt="image" src="https://github.com/user-attachments/assets/7a7074dd-0bd7-41b0-a695-a8208266befd" />

### 7. 컨텐츠 저장소
HTML문서를 저장하는 시스템으로 데이터양이 많으므로 대부분 디스크에 저장한다.
인기있는 콘텐츠는 메모리에 두어 접근 지연시간을 줄인다.

### 8. UTL추출기
UTL추출기는 HTML페이지를  파싱하여 링크들을 골라낸다. 
이떄 상대경로를 모두 절대경로로 변환한다.

### 9. URL필터
특정 타입의 콘텐츠나 파일 확장자를 가지는 URL,  접속오류가 발생하거나 접근제외목록에 해당하는 URL들을
크롤링 대상에서 배제하는 역할을 한다.

### 10. 이미 방문한 URL?
보관된 URL을 추적하여 같은 URL을 다시방문하는 것을 방지하고, 시스템이 무한루프에 빠지는 것을 방지한다.

### 11. URL저장소
이미 방문한 URL을 저장한다.

### 웹 크롤러 작업흐름 
<img width="1080" height="785" alt="image" src="https://github.com/user-attachments/assets/b1d3e795-6fe6-4205-8285-b31fe9cd4da3" />

## 3단계 - 상세 설계

### BFS vs DFS
웹은 유향그래프와 같다. 
페이지를 노드, URL을 에지라고 생각하면 된다.
여기서 DFS를 쓸 수도 있지만, 깊이우선탐색을 쓰면 어느정도로 깊숙이 갈지 가늠하기 어려우므로 대부분 BFS, FIFO방식으로 큐를 이용하여 탐색한다.
하지만 BFS또한 두가지 문제점이 있디.
1. 한페이지에서 나오는 링크의 대부분은 같은 내부 서버로 돌아가므로 병렬처리 시 과부화가 생길 수 있다 (예의없는 크롤러로 간주된다)

2. 표준화 된 BFS 알고리즘은 URL에 우선순위가 없어 처리가 공평하다.
 하지만 모든 웹페이지가 같은 수준의 품질을 가지고 있지 않으므로
페이지 순위,사용자 트래픽 양, 업데이트 빈도등을 고려하여 우선순위를 세우는 것이 온당하다.

### 미수집 URL저장소
미수집 URL저장소를 사용하여 위 문제들을 해결할 수 있다.
URL저장소로 예의갖춘 크롤러, URL사이 우선순위를 둘 수 있다는 것.

#### 예의 
웹 크롤러가 짧은 시간안에 같은 서버에 여러번 요청을 보내면 무례한 일이된다.
또한 DoS공격으로 간주되기까지한다.     
따라서 원칙은 동일한 웹 사이트는 한번에 한 페이지만 요청한다는 것이다.     
이를 만족시키기 위해서, 큐를 두어 같은 호스트명을 가지면 같은 큐에 넣어서 처리한다.    
큐 라우터 : 같은 호스트에 속한 URL을 같은 큐에 넣는다.    
매핑 테이블: 호스트 이름과 큐사이의 관계를 보관하는 테이블    
FIFO 큐: 같은 호스트에 속한 URL을 보관한다.     
큐 선택기 : 우선순위에 따라 큐의 URL을 선택    
작업 스레드: 전달된 URL의 HTML을 다운로드 한다.    

### 우선순위
유용성에 따라 URL의 우선순위를 나눌 때 페이지랭크, 트래픽 양, 갱신 빈도등을 확인한다.
순위결정장치:  우선순위를 계산한다.
큐: 우선순위별로 큐가 하나씩 할당된다.
큐 선택기: 임의 큐에서 처리할 URL을 꺼내는 역할을 담당한다.

전면 큐 : 우선순위 결정 
후면 큐 : 크롤러가 예의바르게 행동하도록 동작을 보장

### 신선도
페이지의 수정,삭제 등의 최신상태에서의 데이터를 알기위해 재수집한다.
- 웹 페이지 중 변경이력을 확인
- 우선순위를 두어 중요한 페이지를 좀 더 자주 재수집한다.

### 미수집 URL저장소를 위한 지속성 처리장치
검색 엔진의 크롤러는 수 억개의 URL을 처리하므로 모두 메모리에 올릴 수 없고 그렇다고 모두 디스크에 두면 오버헤드가 크다      
따라서 대부분 URL을 메모리에 두되, IO비용을 줄이기 위해 메모리버퍼에 큐를 두어 버퍼에 든 URL을 주기적으로 디스크에 기록하는 방식이 가능.

### HTML 다운로더
HTTP프로토콜으르 통해 HTML을 다운로드한다.
이과정에서 로봇 제외 프로토콜이 사용된다.
#### 로봇제외 프로토콜?
웹사이트가 크롤러와 소통하는 표준적인 방법으로 파일소에 크롤러가 수집해도 되는 페이지 목록을 가지고 있다.
또한 파일을 연거푸 다운로드하는 것을 피하기위해 파일을 주기적으로 다시받아 캐시에 보관한다.

#### 성능 최적화
HTML의 성능을 최적화하기 위해서
1. 분산크롤링 - 크롤링 작업을 분산시킨다.
2. 도메인 이름 변환 결과 캐시 - 크롤러 성능의 병목이기도 한부분이라 도메인과 IP주소관계를 캐시해서 보관한다.
3. 지역성 - 가까운 서버끼리의 소통이 더 빠르기에 다운로드시간을 줄이기위해 지역적으로 가까이있는 서버 이용
4. 짧은 타임아웃 - 지체되지않도록 최대 타임아웃 시간을 정해둔다.
5. 안정성- 안정해시, 크롤링 상태 및 수집 데이터 저장, 예외처리, 데이터 검증 등을 다룬다.
6. 확장성- 진화하는 시스템을 우해 새로운 형태의 콘텐츠도 쉽게 지원하도록 신경쓴다.
7. 문제있는 콘텐츠 감지 및 회피
     - 1. 중복콘텐츠      
     - 2. 거미덫(크롤러가 무한루프에 빠지도록 서계한 웹페이지 ) -URL길이에 한게를두거나 크롤러 대상에서 URL필터를 두어 제외한다.      
     - 3. 데이터노이즈 - 가치없는 광고나 스팸 URL들을 제외한다.     

## 4단계 - 마무리
논의를 더 깊에 하기위한 주제는 아래와같다.
1. 서버 측 렌더링 -  많은 사이트가 동적으로 페이지링크를 즉석으로 만드므로 동적렌더링을 적용한뒤 페이지를 파싱한다.
2. 원치않는 페이지 필터링 - 크롤링에 사용되는 자원은 유한하므로 조악하거나 스팸성 페이지를 걸러낸다.
3. 데이터 베이스 다중화 및 샤딩- 데이터계층 가용성, 규모확장성, 안정성이 향상된다.
4. 수평적 규모확장성 - 무상태 서버로 만들어아한다.
